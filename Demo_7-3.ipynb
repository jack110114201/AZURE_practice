{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行影像說明服務操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Describe an image - remote =====\n",
      "Description of remote image: \n",
      "'an ancient city with many ruins with Colosseum in the background' with confidence 33.80%\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Set API key.\n",
    "subscription_key = 'API key'\n",
    "# Set endpoint.\n",
    "endpoint = 'endpoint'\n",
    "# Call API\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "remote_image_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/landmark.jpg\"\n",
    "\n",
    "'''\n",
    "Describe an Image - remote\n",
    "This example describes the contents of an image with the confidence score.\n",
    "'''\n",
    "print(\"===== Describe an image - remote =====\")\n",
    "# Call API\n",
    "description_results = computervision_client.describe_image(remote_image_url )\n",
    "\n",
    "# Get the captions (descriptions) from the response, with confidence level\n",
    "print(\"Description of remote image: \")\n",
    "if (len(description_results.captions) == 0):\n",
    "    print(\"No description detected.\")\n",
    "else:\n",
    "    for caption in description_results.captions:\n",
    "        print(\"'{}' with confidence {:.2f}%\".format(caption.text, caption.confidence * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本機端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Describe an image - remote =====\n",
      "Description of remote image: \n",
      "'a group of bottles with white liquid' with confidence 37.83%\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import os\n",
    "\n",
    "# Set API key.\n",
    "subscription_key = 'API key'\n",
    "# Set endpoint.\n",
    "endpoint = 'endpoint'\n",
    "# Call API\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "#本地端\n",
    "# 指定圖檔\n",
    "local_image_path = os.getcwd() + '/_115509216_whatsubject.jpg'\n",
    "\n",
    "# 讀取圖片\n",
    "# 照片要讀成binary\n",
    "local_image = open(local_image_path, \"rb\")\n",
    "\n",
    "'''\n",
    "Describe an Image - remote\n",
    "This example describes the contents of an image with the confidence score.\n",
    "'''\n",
    "print(\"===== Describe an image - remote =====\")\n",
    "# Call API\n",
    "description_results = computervision_client.describe_image_in_stream(local_image)\n",
    "\n",
    "# Get the captions (descriptions) from the response, with confidence level\n",
    "print(\"Description of remote image: \")\n",
    "if (len(description_results.captions) == 0):\n",
    "    print(\"No description detected.\")\n",
    "else:\n",
    "    for caption in description_results.captions:\n",
    "        print(\"'{}' with confidence {:.2f}%\".format(caption.text, caption.confidence * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行臉部偵測服務操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Detect Faces - remote =====\n",
      "Faces in the remote image: \n",
      "'Male' of age 39 at location 118, 159, 212, 253\n",
      "'Male' of age 54 at location 492, 111, 582, 201\n",
      "'Female' of age 55 at location 18, 153, 102, 237\n",
      "'Female' of age 33 at location 386, 166, 467, 247\n",
      "'Female' of age 18 at location 235, 158, 311, 234\n",
      "'Female' of age 8 at location 323, 163, 391, 231\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Set API key.\n",
    "subscription_key = 'API key'\n",
    "# Set endpoint.\n",
    "endpoint = 'endpoint'\n",
    "# Call API\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "\n",
    "'''\n",
    "Detect Faces - remote\n",
    "This example detects faces in a remote image, gets their gender and age, \n",
    "and marks them with a bounding box.\n",
    "'''\n",
    "print(\"===== Detect Faces - remote =====\")\n",
    "# Get an image with faces\n",
    "remote_image_url_faces = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/faces.jpg\"\n",
    "# Select the visual feature(s) you want.\n",
    "# faces 臉部偵測設定\n",
    "remote_image_features = [\"faces\"]\n",
    "# Call the API with remote URL and features\n",
    "detect_faces_results_remote = computervision_client.analyze_image(remote_image_url_faces, remote_image_features)\n",
    "\n",
    "# Print the results with gender, age, and bounding box\n",
    "print(\"Faces in the remote image: \")\n",
    "if (len(detect_faces_results_remote.faces) == 0):\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for face in detect_faces_results_remote.faces:\n",
    "        print(\"'{}' of age {} at location {}, {}, {}, {}\".format(face.gender, face.age, \\\n",
    "        face.face_rectangle.left, face.face_rectangle.top, \\\n",
    "        face.face_rectangle.left + face.face_rectangle.width, \\\n",
    "        face.face_rectangle.top + face.face_rectangle.height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本地端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Detect Faces - remote =====\n",
      "Faces in the remote image: \n",
      "'Male' of age 47 at location 539, 98, 643, 202\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import os\n",
    "\n",
    "# Set API key.\n",
    "subscription_key = 'API key'\n",
    "# Set endpoint.\n",
    "endpoint = 'endpoint'\n",
    "# Call API\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "\n",
    "'''\n",
    "Detect Faces - remote\n",
    "This example detects faces in a remote image, gets their gender and age, \n",
    "and marks them with a bounding box.\n",
    "'''\n",
    "print(\"===== Detect Faces - remote =====\")\n",
    "\n",
    "#本地端\n",
    "# 指定圖檔\n",
    "local_image_path = os.getcwd() + '/phpjq1SDq.jpg'\n",
    "\n",
    "# 讀取圖片\n",
    "# 照片要讀成binary\n",
    "local_image = open(local_image_path, \"rb\")\n",
    "\n",
    "\n",
    "# Select the visual feature(s) you want.\n",
    "remote_image_features = [\"faces\"]\n",
    "# Call the API with remote URL and features\n",
    "detect_faces_results_remote = computervision_client.analyze_image_in_stream(local_image, remote_image_features)\n",
    "\n",
    "# Print the results with gender, age, and bounding box\n",
    "print(\"Faces in the remote image: \")\n",
    "if (len(detect_faces_results_remote.faces) == 0):\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for face in detect_faces_results_remote.faces:\n",
    "        print(\"'{}' of age {} at location {}, {}, {}, {}\".format(face.gender, face.age, \\\n",
    "        face.face_rectangle.left, face.face_rectangle.top, \\\n",
    "        face.face_rectangle.left + face.face_rectangle.width, \\\n",
    "        face.face_rectangle.top + face.face_rectangle.height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行影像類別偵測服務操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以REST API方式取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"imageType\": {\"clipArtType\": 0, \"lineDrawingType\": 0}, \"requestId\": \"ddc4ac04-b316-4437-9b7c-94e0b6d6a79f\", \"metadata\": {\"height\": 600, \"width\": 450, \"format\": \"Jpeg\"}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# If you are using a Jupyter notebook, uncomment the following line.\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "import json\n",
    "#from PIL import Image\n",
    "#from io import BytesIO\n",
    "\n",
    "# Set API key.\n",
    "subscription_key = 'subscription_key'\n",
    "# Set endpoint.\n",
    "endpoint = 'endpoint'\n",
    "\n",
    "analyze_url = endpoint + \"vision/v2.1/analyze\"\n",
    "\n",
    "# Set image_url to the URL of an image that you want to analyze.\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/\" + \\\n",
    "    \"Broadway_and_Times_Square_by_night.jpg/450px-Broadway_and_Times_Square_by_night.jpg\"\n",
    "\n",
    "headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "params = {'visualFeatures': 'imageType'}\n",
    "data = {'url': image_url}\n",
    "response = requests.post(analyze_url, headers=headers,\n",
    "                         params=params, json=data)\n",
    "response.raise_for_status()\n",
    "\n",
    "# The 'analysis' object contains various fields that describe the image. The most\n",
    "# relevant caption for the image is obtained from the 'description' property.\n",
    "print(json.dumps(response.json()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
